# H-M-Personalized-Fashion-Recommendation-System

This project is part of DS5230 coursework, Unsupervised Machine Learning project.
Professor Yehoshua Roi

Kaggle Challenge - https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations

Transaction_subset file, recommendations file are greater than 300 MB and has not been added however, they can be exported from the code.


Libraries used:

Matplotlib, Seaborn for visualizations

Pandas, Numpy for data wrangling

sklearn for train_test_split 

turicreate for  data modelling


We obtained a MAP@K value of 89.5% on an average, it might however vary from 0.72 to 0.90 depending on the sample subset we obtain and the test data we aim to test.




Abstract: 

Over the past few years, the world of machine learning has introduced new perspectives in each of the domains. Recommendations have taken a leap in enhancements since the time they were introduced. Developments have begun to stagnate, which is why most leading companies have started seeking assistance and exploring ways to improve their existing algorithms in the form of challenges, allowing enthusiasts to explore real-world data while the companies benefit from optimizations. Our objective here is to explore the H&M fashion dataset and build a personalized recommendation system using only customer, transactions, and article data which improves user experience. The given dataset included customer data, product data, transaction data, images, and metadata of the products. The challenge becomes interesting with data open to CV with images and NLP with metadata. We employed data exploration techniques and market basket analysis to understand customers and further implemented collaborative, content-based, and popularity-based filtering on the data. Our major emphasis would be on data processing as there is no satisfactory measure. For ease of computation with large datasets, we have used the library Turicreate. To evaluate the model, we are using a MAP@12 value of 0.895 on the subset data. For future works, we intend to explore the supervised methodologies, Image-based recommendations.


Introduction:

By participating in this Kaggle challenge, we plan to explore new and innovative ways to improve recommendation systems. Since we have H&M publicly available, aim to identify a recommendation system that works well with that data. While we are exploring the possibilities of new recommender machines, we did like to uncover the trends in the dataset. Since the competition was open for quite a long time, there were teams from all over the world, attempting more than one technique, combining better optimization techniques, and checking feasibility of the algorithms for this dataset.

What motivates us here by taking up this challenge is that, we are competing with an ample load of tools and techniques which have been used to complete this challenge, while most others have focused on many techniques involving the dataset and the data modelling aspect of this project, we here are experimenting the usage of new libraries, technologies and methods to complete this challenge, our idea is that if our thoughts/ process can improve the algorithm by 10% we did like to couple it with some of the other techniques and amplify the results. Our initial thoughts to working with this project is to only consider a small portion of the datasets, because of our limited computational power. Beyond the scope of computation, what we did like to discover are the trends and approaches that could make a difference with the recommendations. In this project, we are trying to find new ways to make the computation easier to enable processing of larger datasets, obtain better results by meaningful data preprocessing, uncovering trends within the data. These are some of the layers we planned to incorporate in our project. Even a small fraction like 10% can be more than a regular 10% fraction because we are considering 10% customer data to further consider their complete transactions which were nearly 300,000 transactions and the articles purchased by this lot were nearly 70,000 products.

Our step-by-step approach to achieving this goal would be as follows:
1. Subsetting the customers at random that could give us decent proportions in the transactions and articles.
2. Necessary EDA and market basket analysis to discover the hidden trends and possibilities of understanding them for a deeper understanding of a customer’s preferences.
3. Generating 3 datasets to experiment the methods and ways which are further discussed in background.
4. Experimenting on popularity-based recommendations, content based recommendations, collaborative filtering techniques and different similarity measures.
5. Identify the best performing model using RMSE, the dataset, some hidden trends, understand which models work and why.
6. Since there is no valid out of the box metric, we developed a custom industry standard metric based on the requirement to accurately deliver precision over recommendations based on the customers purchase history. This custom metric is MAP@K, Mean average precision @K where k=12, since we are recommending 12 products to the customer. We intend to follow this as our step-by-step guide with some tweaks along the way. However, the goal here is to build a recommender machine that is capable of recommending products that share close resemblance with the customers past transactions.


Background:

Since the total dataset consists of over 1.3M transactions, we are only considering 10% of it and due to some technical difficulties and experimenting, we had to install a ton of libraries which seemed to be useful and while we were experimenting with libraries that could process large amounts of data while having support to recommendation libraries, we found only a handful of libraries that could do both, one such library was Turicreate, however, turicreate is not supported implicitly, for this very purpose, we have worked on Google colab from the data preprocessing part of the project. The reason being that google colab supported the use of turicreate module implicitly and which did not cause any technical failures with respect to other libraries. Our model is trained under the assumption that the real-
world dataset that the model is given transactions details, articles information and the customer information. As mentioned before the dataset did not have ratings information and we are only considering the transactions part of the data. For this reason, we have taken 3 datasets, as we are experimenting with the data using purchase count in place of purchase count. The dataset we are considering for this experiment will have the customer id, article id, and purchase counts of that article with respect to that user. In the second dataset, we are considering a dummy variable purchase scale with a value of 1, this acts as a bias parameter that all users tend to buy all the products in addition to the purchase frequency. The third dataset is similar to the first one, the major difference is when the purchase counts are scaled using a normalizer. The normalized dataset is computed using the formula.

Scaled target column = current_value – min / (max-min)

The reason we are using 3 datasets is because of we are planning to experiment with the way the recommendations work, this is not a vanilla approach where we use ratings to recommend the products. Since this is a Kaggle challenge, we wish to see how new ways and methods can be used to implemented to recommend products when there is no satisfactory measure especially. We wish to explore the existing algorithms and seek new methods and ways in which we can improvise the recommendations, the thought behind our dataset approach is that we wish to see if we can obtain better results from a normalized dataset, or a dataset with bias (inspired from PageRank algorithm), or a general dataset with no scaling or bias. If there exists an algorithm that works best with the dataset. An important mention about the type of data we are working with, since most of the customer information, product information is encrypted, it does not make sense implicitly for someone who is not actually working with the dataset, but the most effective way to get the most out of what we can do would be to understand the EDA process of the project, because the initial data driven analysis by itself does not contribute much towards the recommendations, however, will give us a good understanding of the data, what kind of data it is etc. In addition to these task oriented challenges, we also have to deal with the regular problems like the imbalanced data, it is always possible that the 10% fraction of the data from the parent data, can have some users with most transactions and some users with minority of the transactions. To address the challenges of turicreate, we are partially changing our environment from local to cloud based google colab environment, we have saved our subset dataset and then exported it to google colab to make sure there is synchronicity between both the versions.


Related Work:

There is a lot of scope and ongoing research that is going on in the field of recommender systems. On a higher level, all the recommendation system techniques ideally fall into one of the following categories. 1. Content based filtering, 2. Collaborative filtering and 3. Popularity based recommendation. Content based filtering mainly aims at recommending products based on the customer’s transaction history or their profile and is the best way to recommend personalized.
products. Only drawback with content-based filtering is that it is not helpful for new users and for it work efficiently it expects that there is some sort of profile information to recommend this problem is referred to as cold start problem. Collaborative filtering is based on the idea that people who like stuff or have similar purchase history and satisfaction with them will lead to a trend in purchases. These similarities can be calculated either by item-item or user-user. Popularity based recommendation systems are always incorporated in multiple e-commerce websites or applications to provide the recommendations based on the total number of purchases over a certain period.

In addition to these 3 parent libraries, there are plenty of libraries that support different variants of recommendation systems. While there are numerous libraries that support the functioning of recommendation systems and their implementations, there are not more than a handful of libraries that propagate efficiency into the process of recommendation systems. One such library that can also assist with the working of larger datasets is Turicreate, this library has options that can support the functionality of recommendation systems while also working on optimizing the computation for larger datasets. The most used measures required to compute collaborative filtering are item-item similarities and user-user similarities are Cosine similarity measure, Pearson correlation measure and spatial distance measure. Cosine similarity is based on the angle between two product vectors while the Pearson correlation is computed by overall product similarity with respect to the mean averages and spatial distance measures are computed by calculating pair wise distances between all the points to form a distance matrix grid and thereby may result in a bit of over computation when compared to Pearson or cosine similarity. Where we compete against the world for this challenge is the implementation of optimized datasets and our technique to see if this would enhance the optimization and in general provide much better recommendations and the validation over MAP@K, earlier known challengers have not used MAP@K or in fact any other dedicated metric to evaluate their model. From paper 1 (Hyunwoo.H et al. 2018), our thoughts were driven that we would have the best results for either item based Collaborative Filtering or Content Based Filtering, which is when we then proceeded to explore more research papers and planned on exploring the underlying unsupervised aspect for this challenge. Our reference paper 2(Maria.A et al.2019) inspired us to consider popularity-based recommendations, which propelled us to work on trends based on popularity and their recommendations.

Project Desccription:

We have a dataset consisting of 5 files, customers data, transactions data, articles data, images of over 10000 products, and metadata information. Due to computational overhead time constraints, we have taken only 10% customers from the customers data. And we have considered only the transactions made by these customers, and articles involved in these transactions. We have obtained around 288K transactions and 68K products and 28k customers. This is the subset of the data which we will be working on, and this dataset is hereby referred to as the subset data. We save this subset and use this as dataset whenever we change the working environment due to the technical difficulties and computational limitations. From the subsets, we visualized the distribution of products over its subcategories and across the dataset. We were curious about what age group of customers made most purchases and decided to visualize the customers data. We noticed that most of the customers purchased were from the age range 18 to 32. We can deduce that the best age for the online purchases at H&M is 18 to 32 and beyond this point, it is again between the ages of 45 to 55. These are the two primes for the online H&M online purchases. This will form a basis for the future work for us. At this phase of the project this distribution is not much relevant to us.We perform analysis on the data to understand how distributed the products are and the categories they belong to. The goal of this analysis was to understand the given subset on a deeper level. Our approach is backed by the thought that the better we understand our target audience, the better we can recommend products to them. With this thought in mind, we have visualized the distribution of customers by age, outliers by price, five most expensive items, five least expensive items in the subset. Doing this sort of analysis will allow us to understand and communicate with the data much more easily.
After our initial EDA, we found out that most of the images involved in our subset are not part of provided dataset, and beyond this point, we decided not to operate with images as the images involved in our subset are too low and cannot fully meet scope of this project, hence we only deal with the customers, transactions, and their articles data.

We further performed market basket analysis to uncover the frequent patterns in transactions. This as an independent phase of the project would still make sense but considering the nature of the dataset most of the customers information and the transaction information. Hence, we have only used this to better understand the customers in general and not as a deep dive into understanding the patterns. This phase is successful if the custom-built function can identify what are the frequent purchases made by a customer. Once we were done with the EDA, we worked on generating the 3 datasets discussed in the background and the reasons behind why we needed 3 datasets. One major challenge we incurred at this point was that the installation of the turireate module, the module is not supported by most of the local jupyter notebook environments due to the way it operates, and the installation of this module in the google colab notebook which was our secondary environment. We implemented popularity based recommendations to obtain the most bought products over the pool of transactions. This was our baseline model; we then implemented the content based filtering and collaborative filtering. All the three algorithms were run against the 3 datasets and the 3 models were run against the test dataset to find the best recommendations. We used RMSE metric to evaluate our models. However, we plan to evaluate our best model using MAP@K at K=12, as we are recommending 12 products, in addition to that, we are taking up the 8 most bought products from the market basket analysis we did for that user, use those products to compare against our predicts from the best model identified. This way we were able to obtain a MAP@12 value of 0.895, which gave us good predictions.
